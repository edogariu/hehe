{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a8d52b",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "### In this notebook we tackle the problem of sparsity in the feature representations of the different modes (chromatin accessibility, gene expression, surface protein levels). \n",
    "\n",
    "As noted in https://www.kaggle.com/code/leohash/complete-eda-of-mmscel-integration-data/notebook, DNA data has between 1-30k of the 229k features being nonzero, RNA data has 2-8k of the ~28k features as nonzero, and protein data has a small number of features and is sparse, which means this notebook don't care :)\n",
    "\n",
    "We will apply PCA (and maybe some other techniques) to investigate whether we can usefully hop into a lower-dimensional, densely-populated representation for either of these two modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "debe6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs\n",
    "# !env/bin/pip3 install tables\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import h5py; import hdf5plugin\n",
    "\n",
    "# set paths\n",
    "DATA_DIR = \"data/\"\n",
    "FP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n",
    "\n",
    "FP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\n",
    "FP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\n",
    "FP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n",
    "\n",
    "FP_MULTIOME_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\n",
    "FP_MULTIOME_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\n",
    "FP_MULTIOME_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n",
    "\n",
    "FP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\n",
    "FP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4bb621",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36e89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class h5Loader():\n",
    "    def __init__(self, f: str, batch_size: int, shuffle=False):\n",
    "        \"\"\"\n",
    "        Creates iterator through h5 without loading it all into memory. Because they are big :)\n",
    "        \"\"\"\n",
    "        self.d = h5py.File(f, 'r')[f.split('.')[0].split('/')[1]]['block0_values']\n",
    "        self.batch_size = batch_size\n",
    "        self.length = len(self.d)\n",
    "        self.i = 0\n",
    "        self.shuffle = shuffle\n",
    "        self.idxs = np.random.permutation(self.length // self.batch_size) if self.shuffle else np.arange(self.length // self.batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.d[idx: idx + self.batch_size]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        self.idxs = np.random.permutation(self.length // self.batch_size) if self.shuffle else np.arange(self.length // self.batch_size)\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.i >= len(self.idxs):\n",
    "            raise StopIteration\n",
    "        idx = self.idxs[self.i]\n",
    "        ret = self.d[idx * self.batch_size: idx * self.batch_size + self.batch_size]\n",
    "        self.i += 1\n",
    "        return ret\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be75fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import tqdm\n",
    "\n",
    "class PCA():\n",
    "    def __init__(self, n_components: int):\n",
    "        self.n_components = n_components\n",
    "        self.ipca = IncrementalPCA(n_components, whiten=False)\n",
    "    \n",
    "    def fit(self, filename: str, batch_size=1000, limit=1e9):\n",
    "        \"\"\"\n",
    "        Fits PCA to data stored in the .h5 file given.\n",
    "        Uses incremental PCA and batching for certain memory reasons.\n",
    "        \"\"\"\n",
    "        batch_size = max(batch_size, self.n_components)\n",
    "        l = h5Loader(filename, batch_size, shuffle=limit == 1e9)\n",
    "        \n",
    "        print('Fitting with a batch size of {}!'.format(batch_size))\n",
    "        for i, batch in enumerate(tqdm.tqdm(l)):\n",
    "            if i == limit: break\n",
    "            self.ipca.partial_fit(batch)\n",
    "        \n",
    "    def evaluate(self, filename: str, batch_size=1000, limit=1e9):\n",
    "        \"\"\"\n",
    "        Evaluates reconstruction error of PCA fit with data stored in the .h5 file given.\n",
    "        Uses batching for certain memory reasons.\n",
    "        \"\"\"\n",
    "        batch_size = max(batch_size, self.n_components)\n",
    "        l = h5Loader(filename, batch_size, shuffle=limit == 1e9)\n",
    "        \n",
    "        print('Evaluating with a batch size of {}!'.format(batch_size))\n",
    "        errs = 0.\n",
    "        for i, batch in enumerate(tqdm.tqdm(l)):\n",
    "            if i == limit: break\n",
    "            t = self.ipca.transform(batch)\n",
    "            b_hat = self.ipca.inverse_transform(t)\n",
    "            diffs = np.linalg.norm(b_hat - batch, axis=-1)\n",
    "            errs += diffs.sum()\n",
    "        errs /= batch_size * min(limit, len(l))\n",
    "        return errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336cc56",
   "metadata": {},
   "source": [
    "# Da Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b473da",
   "metadata": {},
   "source": [
    "## DNA (Chromatin Accessibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f93dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = FP_MULTIOME_TRAIN_INPUTS\n",
    "test_f = FP_MULTIOME_TEST_INPUTS\n",
    "\n",
    "max_n_components = 20000\n",
    "\n",
    "p_dna = PCA(max_n_components)\n",
    "p_dna.fit(train_f)\n",
    "p_dna.fit(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7c33e",
   "metadata": {},
   "source": [
    "## RNA (Gene Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = FP_CITE_TRAIN_INPUTS\n",
    "\n",
    "p10k_rna = PCA(10000)\n",
    "p10k_rna.fit(train_f)\n",
    "p10k_rna.evaluate(train_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c015dfc",
   "metadata": {},
   "source": [
    "# random test bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38839642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4578840128 <class 'numpy.ndarray'>\n",
      "112 <class 'scipy.sparse._arrays.csr_array'>\n",
      "878.241132583\n"
     ]
    }
   ],
   "source": [
    "import h5sparse as h5\n",
    "import scipy.sparse as ss\n",
    "import sys\n",
    "\n",
    "h5f = h5.File(FP_MULTIOME_TEST_INPUTS)\n",
    "\n",
    "normalboi = h5f['test_multi_inputs']['block0_values'][:5000]\n",
    "print(sys.getsizeof(normalboi), normalboi.__class__)\n",
    "sparseboi = ss.csr_array(normalboi)\n",
    "print(sys.getsizeof(sparseboi.data), sparseboi.__class__)\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import time\n",
    "\n",
    "p1 = PCA(5000)\n",
    "p2 = PCA(5000)\n",
    "t = TruncatedSVD(5000)\n",
    "\n",
    "\n",
    "s = time.perf_counter()\n",
    "t.fit(sparseboi)\n",
    "print(time.perf_counter() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aedf7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with a batch size of 5000!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 11/11 [18:57<00:00, 103.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147.24937657248756"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(filename: str, batch_size=1000, limit=1e9):\n",
    "    \"\"\"\n",
    "    Evaluates reconstruction error of PCA fit with data stored in the .h5 file given.\n",
    "    Uses batching for certain memory reasons.\n",
    "    \"\"\"\n",
    "    batch_size = max(batch_size, 5000)\n",
    "    l = h5Loader(filename, batch_size, shuffle=limit == 1e9)\n",
    "\n",
    "    print('Evaluating with a batch size of {}!'.format(batch_size))\n",
    "    errs = 0.\n",
    "    for i, batch in enumerate(tqdm.tqdm(l)):\n",
    "        if i == limit: break\n",
    "        tr = t.transform(batch)\n",
    "        b_hat = t.inverse_transform(tr)\n",
    "        diffs = np.linalg.norm(b_hat - batch, axis=-1)\n",
    "        errs += diffs.sum()\n",
    "    errs /= batch_size * min(limit, len(l))\n",
    "    return errs\n",
    "evaluate(FP_MULTIOME_TEST_INPUTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonbloom-env",
   "language": "python",
   "name": "jonbloom-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
