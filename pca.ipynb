{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a8d52b",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "### In this notebook we tackle the problem of sparsity in the feature representations of the different modes (chromatin accessibility, gene expression, surface protein levels). \n",
    "\n",
    "As noted in https://www.kaggle.com/code/leohash/complete-eda-of-mmscel-integration-data/notebook, DNA data has between 1-30k of the 229k features being nonzero, RNA data has 2-8k of the ~28k features as nonzero, and protein data has a small number of features and is sparse, which means this notebook don't care :)\n",
    "\n",
    "We will apply PCA (and maybe some other techniques) to investigate whether we can usefully hop into a lower-dimensional, densely-populated representation for either of these two modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from datasets import SparseDataset, H5Dataset\n",
    "\n",
    "from sklearn.decomposition import PCA, SparsePCA, IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336cc56",
   "metadata": {},
   "source": [
    "# Da Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b473da",
   "metadata": {},
   "source": [
    "## DNA (Chromatin Accessibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f93dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 207/207 [03:56<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# find max variance columns\n",
    "from datasets import H5Dataset, SparseDataset\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "sums = np.zeros(228942)\n",
    "squared_sums = np.zeros(228942)\n",
    "num_nonzero = np.zeros(228942)\n",
    "\n",
    "d = SparseDataset('all', 'multi')\n",
    "s = d.get_dataloader(512)\n",
    "for x, day, y in tqdm.tqdm(s):\n",
    "    sums += x.sum(dim=0).numpy()\n",
    "    squared_sums += torch.square(x).sum(dim=0).numpy()\n",
    "    num_nonzero += (x != 0).sum(dim=0).numpy()\n",
    "    \n",
    "variances = squared_sums / 105942 - np.square(sums / 105942)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cb5e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_idxs = np.argsort(variances)[::-1]\n",
    "nz_idxs = np.argsort(num_nonzero)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c431663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.zeros(228942)\n",
    "for i, v in enumerate(var_idxs):\n",
    "    ranks[v] += i\n",
    "for i, v in enumerate(nz_idxs):\n",
    "    ranks[v] += i\n",
    "best_idxs = np.argsort(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4869e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/multi_best_idxs.npy', best_idxs)\n",
    "np.save('data/multi_var_idxs.npy', var_idxs)\n",
    "np.save('data/multi_nz_idxs.npy', nz_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7c33e",
   "metadata": {},
   "source": [
    "## RNA (Gene Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230502fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "multi_df = pd.read_hdf('data/train_multi_targets.h5', start=1000, stop=2000)\n",
    "cite_df = pd.read_hdf('data/train_cite_inputs.h5', start=1000, stop=2000)\n",
    "\n",
    "multi_keys = list(multi_df.keys())\n",
    "cite_keys = list(cite_df.keys())\n",
    "\n",
    "for i in range(len(cite_keys)):\n",
    "    cite_keys[i] = cite_keys[i].split('_')[0]\n",
    "\n",
    "multi_idxs = []\n",
    "cite_idxs = []\n",
    "for i, s in enumerate(multi_keys):\n",
    "    if s in cite_keys:\n",
    "        multi_idxs.append(i)\n",
    "        cite_idxs.append(cite_keys.index(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_rna = np.asarray(H5Dataset('all', 'multi').targets_h5)\n",
    "# cite_rna = np.asarray(H5Dataset('all', 'cite').inputs_h5)\n",
    "# multi_shared = multi_rna[:, multi_idxs]\n",
    "# cite_shared = cite_rna[:, cite_idxs]\n",
    "# # shared = np.concatenate((multi_shared, cite_shared), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10594a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5120\n",
    "\n",
    "multi_shared_loader = H5Dataset('all', 'multi').get_dataloader(batch_size)\n",
    "cite_shared_loader = H5Dataset('all', 'cite').get_dataloader(batch_size)\n",
    "\n",
    "p = IncrementalPCA(5000, batch_size=batch_size)\n",
    "\n",
    "for (x, day), y in tqdm.tqdm(multi_shared_loader):\n",
    "    rna = y.numpy()\n",
    "    rna = rna[:, multi_idxs]\n",
    "    p.fit((rna != 0).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f98d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (x, day), y in tqdm.tqdm(multi_shared_loader):\n",
    "#     rna = y.numpy()\n",
    "#     rna = (rna[:, multi_idxs] != 0).astype(float)\n",
    "#     t = p.transform(rna)\n",
    "#     r = p.inverse_transform(t)\n",
    "#     print(r[0, :20])\n",
    "#     print(rna[0, :20])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, day), y in tqdm.tqdm(cite_shared_loader):\n",
    "    rna = x.numpy()\n",
    "    rna = rna[:, cite_idxs]\n",
    "    p.fit((rna != 0).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d235f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, day), y in tqdm.tqdm(multi_shared_loader):\n",
    "    rna = y.numpy()\n",
    "    rna = (rna[:, multi_idxs] != 0).astype(float)\n",
    "    t = p.transform(rna)\n",
    "    r = p.inverse_transform(t)\n",
    "    print(r[0, :20])\n",
    "    print(rna[0, :20])\n",
    "    break\n",
    "for (x, day), y in tqdm.tqdm(cite_shared_loader):\n",
    "    rna = x.numpy()\n",
    "    rna = (rna[:, cite_idxs] != 0).astype(float)\n",
    "    t = p.transform(rna)\n",
    "    r = p.inverse_transform(t)\n",
    "    print(r[0, :20])\n",
    "    print(rna[0, :20])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = IncrementalPCA(5120)\n",
    "with open('data/pca.pkl', 'rb') \n",
    "p2 = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
