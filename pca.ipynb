{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a8d52b",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "### In this notebook we tackle the problem of sparsity in the feature representations of the different modes (chromatin accessibility, gene expression, surface protein levels). \n",
    "\n",
    "As noted in https://www.kaggle.com/code/leohash/complete-eda-of-mmscel-integration-data/notebook, DNA data has between 1-30k of the 229k features being nonzero, RNA data has 2-8k of the ~28k features as nonzero, and protein data has a small number of features and is sparse, which means this notebook don't care :)\n",
    "\n",
    "We will apply PCA (and maybe some other techniques) to investigate whether we can usefully hop into a lower-dimensional, densely-populated representation for either of these two modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ad206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from datasets import SparseDataset, H5Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = SparseDataset('train', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dbe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import tqdm\n",
    "\n",
    "class PCA():\n",
    "    def __init__(self, n_components: int):\n",
    "        self.n_components = n_components\n",
    "        self.ipca = IncrementalPCA(n_components, whiten=False)\n",
    "    \n",
    "    def fit(self, filename: str, batch_size=1000, limit=1e9):\n",
    "        \"\"\"\n",
    "        Fits PCA to data stored in the .h5 file given.\n",
    "        Uses incremental PCA and batching for certain memory reasons.\n",
    "        \"\"\"\n",
    "        batch_size = max(batch_size, self.n_components)\n",
    "        l = h5Loader(filename, batch_size, shuffle=limit == 1e9)\n",
    "        \n",
    "        print('Fitting with a batch size of {}!'.format(batch_size))\n",
    "        for i, batch in enumerate(tqdm.tqdm(l)):\n",
    "            if i == limit: break\n",
    "            self.ipca.partial_fit(batch)\n",
    "        \n",
    "    def evaluate(self, filename: str, batch_size=1000, limit=1e9):\n",
    "        \"\"\"\n",
    "        Evaluates reconstruction error of PCA fit with data stored in the .h5 file given.\n",
    "        Uses batching for certain memory reasons.\n",
    "        \"\"\"\n",
    "        batch_size = max(batch_size, self.n_components)\n",
    "        l = h5Loader(filename, batch_size, shuffle=limit == 1e9)\n",
    "        \n",
    "        print('Evaluating with a batch size of {}!'.format(batch_size))\n",
    "        errs = 0.\n",
    "        for i, batch in enumerate(tqdm.tqdm(l)):\n",
    "            if i == limit: break\n",
    "            t = self.ipca.transform(batch)\n",
    "            b_hat = self.ipca.inverse_transform(t)\n",
    "            diffs = np.linalg.norm(b_hat - batch, axis=-1)\n",
    "            errs += diffs.sum()\n",
    "        errs /= batch_size * min(limit, len(l))\n",
    "        return errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336cc56",
   "metadata": {},
   "source": [
    "# Da Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b473da",
   "metadata": {},
   "source": [
    "## DNA (Chromatin Accessibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f93dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = FP_MULTIOME_TRAIN_INPUTS\n",
    "test_f = FP_MULTIOME_TEST_INPUTS\n",
    "\n",
    "max_n_components = 20000\n",
    "\n",
    "p_dna = PCA(max_n_components)\n",
    "p_dna.fit(train_f)\n",
    "p_dna.fit(test_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7c33e",
   "metadata": {},
   "source": [
    "## RNA (Gene Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_f = FP_CITE_TRAIN_INPUTS\n",
    "\n",
    "p10k_rna = PCA(10000)\n",
    "p10k_rna.fit(train_f)\n",
    "p10k_rna.evaluate(train_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c015dfc",
   "metadata": {},
   "source": [
    "# random test bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38839642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5sparse as h5\n",
    "import scipy.sparse as ss\n",
    "import sys\n",
    "\n",
    "h5f = h5.File(FP_MULTIOME_TEST_INPUTS)\n",
    "\n",
    "normalboi = h5f['test_multi_inputs']['block0_values'][:5000]\n",
    "print(sys.getsizeof(normalboi), normalboi.__class__)\n",
    "sparseboi = ss.csr_array(normalboi)\n",
    "print(sys.getsizeof(sparseboi.data), sparseboi.__class__)\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import time\n",
    "\n",
    "p1 = PCA(5000)\n",
    "p2 = PCA(5000)\n",
    "t = TruncatedSVD(5000)\n",
    "\n",
    "\n",
    "s = time.perf_counter()\n",
    "t.fit(sparseboi)\n",
    "print(time.perf_counter() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filename: str, batch_size=1000, limit=1e9):\n",
    "    \"\"\"\n",
    "    Evaluates reconstruction error of PCA fit with data stored in the .h5 file given.\n",
    "    Uses batching for certain memory reasons.\n",
    "    \"\"\"\n",
    "    batch_size = max(batch_size, 5000)\n",
    "    l = h5Loader(filename, batch_size, shuffle=limit == 1e9)\n",
    "\n",
    "    print('Evaluating with a batch size of {}!'.format(batch_size))\n",
    "    errs = 0.\n",
    "    for i, batch in enumerate(tqdm.tqdm(l)):\n",
    "        if i == limit: break\n",
    "        tr = t.transform(batch)\n",
    "        b_hat = t.inverse_transform(tr)\n",
    "        diffs = np.linalg.norm(b_hat - batch, axis=-1)\n",
    "        errs += diffs.sum()\n",
    "    errs /= batch_size * min(limit, len(l))\n",
    "    return errs\n",
    "evaluate(FP_MULTIOME_TEST_INPUTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonbloomenv",
   "language": "python",
   "name": "jonbloomenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
